{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7d09ef-cce2-4532-bf81-55540a63e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from data_prep import load_tfrecord_dataset, TextTransform\n",
    "\n",
    "model = tf.keras.models.load_model('stt_custom_model.h5', compile=False)\n",
    "\n",
    "text_transform = TextTransform()\n",
    "test_dataset = load_tfrecord_dataset(\n",
    "    'test.tfrecord',\n",
    "    batch_size=12,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "idx2char = text_transform.index_map\n",
    "blank_index = len(idx2char)\n",
    "num_samples = 5\n",
    "\n",
    "\n",
    "def ctc_beam_search(log_probs, beam_width=10, blank_idx=None, idx2char=None):\n",
    "    T, V = log_probs.shape\n",
    "    if blank_idx is None:\n",
    "        blank_idx = V - 1\n",
    "\n",
    "    beams = {(\"\", True): 0.0}\n",
    "    for t in range(T):\n",
    "        next_beams = {}\n",
    "        for (pref, last_blank), score in beams.items():\n",
    "            s_blank = score + log_probs[t, blank_idx]\n",
    "            next_beams[(pref, True)] = np.logaddexp(\n",
    "                next_beams.get((pref, True), -1e30), s_blank\n",
    "            )\n",
    "            for c in range(V):\n",
    "                if c == blank_idx:\n",
    "                    continue\n",
    "                ch = idx2char.get(c, \"\")\n",
    "                if not ch:\n",
    "                    continue\n",
    "                if len(pref) > 0 and ch == pref[-1] and not last_blank:\n",
    "                    s_char = score + log_probs[t, c]\n",
    "                    new_key = (pref, False)\n",
    "                else:\n",
    "                    s_char = score + log_probs[t, c]\n",
    "                    new_key = (pref + ch, False)\n",
    "                next_beams[new_key] = np.logaddexp(\n",
    "                    next_beams.get(new_key, -1e30), s_char\n",
    "                )\n",
    "        beams = dict(sorted(next_beams.items(), key=lambda kv: kv[1], reverse=True)[:beam_width])\n",
    "    best = max(beams.items(), key=lambda kv: kv[1])[0][0]\n",
    "    return best\n",
    "\n",
    "def collapse_repeats(text):\n",
    "    collapsed = []\n",
    "    prev = ''\n",
    "    for ch in text:\n",
    "        if ch != prev or ch == ' ':\n",
    "            collapsed.append(ch)\n",
    "        prev = ch\n",
    "    return ''.join(collapsed).strip()\n",
    "\n",
    "for i, (spectrogram, _, _, _) in enumerate(test_dataset.take(num_samples)):\n",
    "    preds = model.predict(spectrogram, verbose=0)[0]\n",
    "    log_probs = np.log(np.clip(preds, 1e-10, 1.0))\n",
    "\n",
    "    decoded_beam = ctc_beam_search(\n",
    "        log_probs,\n",
    "        beam_width=10,\n",
    "        blank_idx=blank_index,\n",
    "        idx2char=idx2char\n",
    "    )\n",
    "\n",
    "    decoded_clean = collapse_repeats(decoded_beam)\n",
    "    print(f\"\\nüü© Sample {i+1}\")\n",
    "    print(f\"Beam search raw: {decoded_beam}\")\n",
    "    print(f\"Collapsed clean: {decoded_clean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f54c9802-e386-456f-ae46-0a4a1e8d6456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from data_prep import load_tfrecord_dataset, TextTransform\n",
    "from model import build_model, ctc_loss\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "train_tfrecord = 'train.tfrecord'\n",
    "test_tfrecord = 'test.tfrecord'\n",
    "input_shape = (128, 1200, 1)\n",
    "batch_size = 12\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee1d7e41-7294-4721-82a2-7fc66077878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_tfrecord_dataset(train_tfrecord, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = load_tfrecord_dataset(test_tfrecord, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "text_transform = TextTransform()\n",
    "vocab_size = len(text_transform.char_map)\n",
    "\n",
    "model = build_model(input_shape, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e35261d6-0b36-45af-8102-794ef04809c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\bekas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\bekas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\legacy\\backend.py:666: The name tf.nn.ctc_loss is deprecated. Please use tf.compat.v1.nn.ctc_loss instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = layers.Input(name='label', shape=(None,), dtype='int32')\n",
    "input_length = layers.Input(name='input_length', shape=(1,), dtype='int64')\n",
    "label_length = layers.Input(name='label_length', shape=(1,), dtype='int64')\n",
    "\n",
    "loss_out = layers.Lambda(lambda x: ctc_loss(*x))([labels, model.output, input_length, label_length])\n",
    "training_model = Model(inputs=[model.input, labels, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate=3e-5, decay_steps=500, decay_rate=0.95)\n",
    "opt = Adam(learning_rate=lr_schedule, clipnorm=5.0, epsilon=1e-7)\n",
    "training_model.compile(optimizer=opt, loss=lambda y_true, y_pred: y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a60c01-2227-4ad0-979c-cf5635ae39e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for spectrogram, label, input_len, label_len in train_dataset.take(1):\n",
    "    print(\"Spectrogram shape:\", spectrogram.shape)\n",
    "    print(\"Label shape:\", label.shape)\n",
    "    print(\"Input lengths:\", input_len.numpy()[:5])\n",
    "    print(\"Label lengths:\", label_len.numpy()[:5])\n",
    "\n",
    "true_len = int(label_len[0].numpy())\n",
    "sample_label = label[0][:true_len].numpy()\n",
    "decoded_text = text_transform.int_to_text(sample_label)\n",
    "print(\"Decoded text:\", decoded_text)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample_spec = spectrogram[0].numpy().squeeze()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.imshow(sample_spec.T, aspect='auto', origin='lower')\n",
    "plt.title(f\"Mel-spektrogram\\nTekst: \\\"{decoded_text.strip()}\\\"\")\n",
    "plt.xlabel(\"Vreme (frame-ovi)\")\n",
    "plt.ylabel(\"Mel frekvencije\")\n",
    "plt.colorbar(label='Intenzitet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befb1d9e-d53a-4e8a-9a14-a32f644d7b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 300\n",
    "train_dataset = train_dataset.repeat()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoha {epoch + 1}/{epochs}\")\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for step, (spectrogram, label, input_len, label_len) in enumerate(train_dataset):\n",
    "        input_len_b = tf.expand_dims(tf.cast(input_len, tf.int64), axis=-1)\n",
    "        label_len_b = tf.expand_dims(tf.cast(label_len, tf.int64), axis=-1)\n",
    "        loss = training_model.train_on_batch(\n",
    "            [spectrogram, label, input_len_b, label_len_b],\n",
    "            np.zeros(len(spectrogram))\n",
    "        )\n",
    "        total_loss += loss\n",
    "        num_batches += 1\n",
    "        if (step + 1) % 20 == 0:\n",
    "            print(f\"  Korak {step + 1}, gubitak(loss): {loss:.4f}\")\n",
    "\n",
    "        if step + 1 >= steps_per_epoch:\n",
    "            break\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Proseƒçan gubitak: {avg_loss:.4f}\")\n",
    "\n",
    "model.save('stt_custom_model.h5')\n",
    "print(\"Model je saƒçuvan kao stt_custom_model.h5\")\n",
    "\n",
    "blank_index = len(text_transform.char_map)\n",
    "for spectrogram, _, _, _ in test_dataset.take(3):\n",
    "    preds = model.predict(spectrogram, verbose=0)\n",
    "    pred_indices = np.argmax(preds, axis=-1)\n",
    "    deduped = []\n",
    "    prev = -1\n",
    "    for idx in pred_indices[0]:\n",
    "        if idx != prev and idx != blank_index:\n",
    "            deduped.append(idx)\n",
    "        prev = idx\n",
    "    tekst = ''.join([text_transform.index_map.get(i, '') for i in deduped])\n",
    "    print(\"\\nPrimer predikcije:\", tekst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2434ecf-d8d3-4c58-b38a-ca330d4ae804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
